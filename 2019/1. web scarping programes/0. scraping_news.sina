# -*- encoding：utf-8 -*-
# @ModuleName: 1. scraping_news.sina
# @Function:
# @Author: sort_man
# @Time: 2019/10/4 15:18

"""熟悉基本规则"""
import requests
newsurl = "https://news.sina.com.cn/china/"   # headers,request url
res = requests.get(newsurl)  # 获取response 下内容
res.encoding = "utf-8"   # 改为中文编码
print(res.text)

from bs4 import BeautifulSoup
html_sample = '\
<html> \
<body> \
<h1 id = "title" >Hello World</h1> \
<a href = "#" class = "link" > This is link1</a> \
<a href = "#" class = "link" > This is link2</a> \
<span class="num" node-type="comment-num">32</span>\
</body> \
</html>'
soup = BeautifulSoup(html_sample,"html.parser")  # 去除干扰标签，取出文本内容
print(soup.text)

header = soup.select("h1")  # select取出对应标签下的值
print(header)
print(header[0])  # 取出第一个元素
print(header[0].text)  # 取出第一个元素中的文本内容

alink = soup.select("a")   # select 取出 a 开头所在行数据
print(alink)
for link in alink:
    print(link)
    print(link.text)

""" 使用select 找出所有id 为title的元素(id前需加#)"""
alink = soup.select("#title")
print(alink)

""" 使用select找出所有class为link的元素（class前面需加.)"""
for link in soup.select(".link"):
    print(link)

""" 使用select找出所有 a tag 的href连结或其他连接属性值"""
alinks = soup.select("a")
print(alinks)
for link in alinks:
    print(link["href"])
    
    





"""取得评论数与评论内容"""
import requests
import json   # python爬虫抓取javascrip动态网页，例如评论数，点评数等
comments = requests.get("https://comment.sina.com.cn/page/info?version=1&\
                        format=json&channel=gn&newsid=comos-icezzrr0168025&\
                        group=undefined&compress=0&ie=utf-8&oe=utf-8&page=1&\
                        page_size=3&t_size=3&h_size=3&thread=1&uid=unlogin_user") # 去除末尾&callback=jsonp_1570244539520&_=1570244539520
# ccc = comments.text.strip()
# print(ccc)
jd = json.loads(comments.text)
print(jd)
print(jd["result"]["count"]["total"])  #  获取总评论数

""" 获取新闻标识符"""
newsurl = "https://news.sina.com.cn/c/2019-10-05/doc-iicezzrr0168025.shtml"
newsid = newsurl.split("/")[-1].rstrip(".shtml").lstrip("doc-i")  # 字符串切割提取新闻标识符
# print(newsid)
import re
m = re.search("doc-i(.*).shtml",newsurl)  # 正则匹配提取新闻标识符
print(m.group(1))  # 取得小括号中文字部分

"""评论数抽取函数"""
import re
import json
def getCommentCounts(newsurl):
    commentURL = "https://comment.sina.com.cn/page/info?version=1&\
                            format=json&channel=gn&newsid=comos-{}&\
                            group=undefined&compress=0&ie=utf-8&oe=utf-8&page=1&\
                           page_size=3&t_size=3&h_size=3&thread=1&uid=unlogin_user"
    m = re.search("doc-i(.*).shtml",newsurl)
    newsid = m.group(1)
    comments = requests.get(commentURL.format(newsid))
    jd = json.loads(comments.text)
    return jd["result"]["count"]["total"]

news1 = "https://news.sina.com.cn/c/2019-10-05/doc-iicezzrr0168025.shtml"
print(getCommentCounts(news1))

""" 完成内文信息抽取函数"""
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import pandas as pd
def getNewsDetail(newsurl):
    result = {}
    res = requests.get(newsurl)
    res.encoding = "utf-8"
    soup = BeautifulSoup(res.text,"html.parser")
    result["title" ]= soup.select(".main-title")[0].text  # 获取标题
    result["newssource"] = soup.select(".source")[0].text   # 获取文章来源地址
    date1 = soup.select(".date")[0].text
    date2 = date1.replace(" ", "")
    result["date"] = datetime.strptime(date2, "%Y年%m月%d日%H:%M")  # 将字符串格式改为时间格式
    result["article"] = " ".join([p.text.strip() for p in soup.select("#article p")[:-1]])  # 功能同上
    result["editor"] = soup.select(".show_author")[0].text.strip("责任编辑：")
    result["comments"] = getCommentCounts(newsurl)   # 调用提取评论数函数
    df = pd.DataFrame([result])
    return result,df
news1 = "https://news.sina.com.cn/c/2019-10-05/doc-iicezzrr0168025.shtml"
print(getNewsDetail(news1))






"""批量提取,找出对应header,提取页数，提取页数中的id,带入内文信息提取函数，遍历，保存”
res =requests.get("https://feed.sina.com.cn/api/roll/get?pageid=121&lid=1356&num=20&versionNumber=1.2.4&page=1")
res =requests.get("https://feed.sina.com.cn/api/roll/get?pageid=121&lid=1356&num=20&versionNumber=1.2.4&page={}")

